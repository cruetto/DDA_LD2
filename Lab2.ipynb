{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c2f1ebfd-d4d1-436d-8eab-dee681b57a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark Session...\n",
      "Spark Session Initialized. Reading data from file: duom_full.txt\n",
      "Loading data from duom_full.txt...\n",
      "Parsing data...\n",
      "Caching parsed data for faster access...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 7) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed and cached 59659 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession # The main entry point for Spark functionality\n",
    "import re # Regular expressions, used here for parsing (though the final version uses splits)\n",
    "import time # To potentially measure execution time (not used for calculation here)\n",
    "\n",
    "# --- Configuration ---\n",
    "file_path = \"duom_full.txt\"\n",
    "\n",
    "# --- Helper Function to Parse Lines ---\n",
    "\n",
    "def parse_line_simple(line_text):\n",
    "    \"\"\"\n",
    "    This function takes one line of text from the input file, which looks like:\n",
    "    '{{key1=value1}{key2=value2}...}'\n",
    "    and tries to convert it into a Python dictionary (a collection of key-value pairs), like:\n",
    "    {'key1': 'value1', 'key2': 'value2', ...}\n",
    "\n",
    "    Args:\n",
    "        line_text (str): A single line from the raw data file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the data in the line, or\n",
    "        None: If the line is empty or cannot be parsed properly.\n",
    "    \"\"\"\n",
    "    record = {} # Create an empty dictionary to store the results\n",
    "    try:\n",
    "        # 1. Basic Check: Ignore empty lines\n",
    "        if not line_text or not line_text.strip():\n",
    "             return None # Return nothing if the line is empty\n",
    "\n",
    "        # 2. Remove Outer Braces: Get rid of the starting '{{' and ending '}}'\n",
    "        #    Example: '{{a=1}{b=2}}' becomes 'a=1}{b=2'\n",
    "        #    We use strip('{') and strip('}') which might remove extra braces if present,\n",
    "        #    but it's simple for this format. A more robust way might use slicing [2:-2].\n",
    "        content = line_text.strip().strip('{}')\n",
    "\n",
    "        # 3. Split into Pairs: Split the string by '}{' to get individual key-value parts\n",
    "        #    Example: 'a=1}{b=2' becomes ['a=1', 'b=2']\n",
    "        parts = content.split('}{')\n",
    "\n",
    "        # 4. Process Each Pair: Loop through each part (like 'a=1')\n",
    "        for part in parts:\n",
    "            # Sometimes the first/last part might still have a stray brace, remove it.\n",
    "            # Example: '{a=1' becomes 'a=1', 'b=2}' becomes 'b=2'\n",
    "            part_cleaned = part.strip('{}')\n",
    "\n",
    "            # Split the part by the FIRST equals sign '=' into key and value\n",
    "            # Example: 'a=1' becomes ['a', '1']\n",
    "            # Using maxsplit=1 ensures that if a value itself contains '=', it's kept together.\n",
    "            key_value = part_cleaned.split('=', 1)\n",
    "\n",
    "            # Check if we successfully got two pieces (a key and a value)\n",
    "            if len(key_value) == 2:\n",
    "                key = key_value[0].strip() # Get the key, remove extra spaces\n",
    "                value = key_value[1].strip() # Get the value, remove extra spaces\n",
    "\n",
    "                # Make sure the key is not empty before adding to the dictionary\n",
    "                if key:\n",
    "                    record[key] = value # Add the key-value pair to our dictionary\n",
    "\n",
    "        # 5. Return Result: Return the dictionary if we added anything, otherwise None\n",
    "        return record if record else None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- Spark Session Initialization ---\n",
    "\n",
    "# Create a SparkSession. This is the main connection to a Spark cluster.\n",
    "# .appName(): Gives your Spark job a name, useful for monitoring.\n",
    "# .master(\"local[*]\"): Tells Spark to run locally using all available CPU cores.\n",
    "#                      This is good for development/testing on a single machine.\n",
    "#                      For a real cluster, you'd use a different master URL.\n",
    "# .getOrCreate(): Gets an existing SparkSession or creates a new one if none exists.\n",
    "print(\"Initializing Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogisticsRDDAnalysis_Simple\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the SparkContext from the SparkSession.\n",
    "# SparkContext (usually named 'sc') is the main entry point for low-level RDD API.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set log level to WARN to see less informational messages from Spark\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Session Initialized. Reading data from file: {file_path}\")\n",
    "\n",
    "# --- Data Loading and Parsing ---\n",
    "\n",
    "# Use try-except to catch errors during file reading\n",
    "try:\n",
    "    # 1. Read the text file into an RDD (Resilient Distributed Dataset).\n",
    "    #    An RDD is Spark's core data structure, representing a collection of items\n",
    "    #    distributed across the cluster nodes, which can be processed in parallel.\n",
    "    #    Each element in this initial RDD is one line (string) from the text file.\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    raw_lines_rdd = sc.textFile(file_path)\n",
    "\n",
    "    # 2. Parse each line using our 'parse_line_simple' function.\n",
    "    #    `.map()` is a *transformation* that applies a function to each element\n",
    "    #    of an RDD and returns a *new* RDD with the results.\n",
    "    #    Input RDD: contains strings (lines from the file)\n",
    "    #    Output RDD: contains dictionaries (parsed data) or None (for bad lines)\n",
    "    print(\"Parsing data...\")\n",
    "    parsed_rdd_with_none = raw_lines_rdd.map(parse_line_simple)\n",
    "\n",
    "    # 3. Filter out the None values (lines that failed parsing).\n",
    "    #    `.filter()` is a *transformation* that keeps only elements for which\n",
    "    #    the given function returns True. `lambda x: x is not None` is a short\n",
    "    #    way to write a function that checks if an element `x` is not None.\n",
    "    #    Input RDD: contains dictionaries and None values\n",
    "    #    Output RDD: contains only dictionaries\n",
    "    parsed_rdd = parsed_rdd_with_none.filter(lambda record: record is not None)\n",
    "\n",
    "    # 4. Cache the parsed RDD in memory.\n",
    "    #    `.cache()` is a *transformation* that tells Spark to store the results\n",
    "    #    of this RDD in memory the first time it's computed.\n",
    "    #    This is very useful because we will use `parsed_rdd` multiple times (for each task).\n",
    "    #    Without caching, Spark would re-read the file and re-parse it every time.\n",
    "    #    Note: Caching is \"lazy\" - it only happens when an *action* (like count or collect) is called.\n",
    "    print(\"Caching parsed data for faster access...\")\n",
    "    parsed_rdd.cache()\n",
    "\n",
    "    # 5. Trigger an action to perform parsing and caching, and check data.\n",
    "    #    `.count()` is an *action* that computes the number of elements in the RDD.\n",
    "    #    Calling an action forces Spark to execute all the planned transformations (`map`, `filter`).\n",
    "    record_count = parsed_rdd.count()\n",
    "    if record_count == 0:\n",
    "        # If no records were parsed, something is wrong (bad path? empty file? bad format?).\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"Warning: No records were successfully parsed from the file: {file_path}\")\n",
    "        print(f\"Please check that the file exists, the path is correct, and the format\")\n",
    "        print(f\"matches the expected '{{key=value}}{{key=value}}...' structure.\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        spark.stop() # Stop Spark\n",
    "        exit() # Exit the script\n",
    "    else:\n",
    "        print(f\"Successfully parsed and cached {record_count} records.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If there was an error reading the file itself (e.g., file not found)\n",
    "    print(f\"Error reading or parsing file '{file_path}': {e}\")\n",
    "    print(\"Please ensure the file exists and the path is correct in the 'file_path' variable.\")\n",
    "    spark.stop()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab4eb0-3577-49dd-9d65-d68490e98f28",
   "metadata": {},
   "source": [
    "### Slenkstinis lygmuo (5-6): Suskaičiuokite mažiausią, didžiausią ir vidutinį (aritmetinis vidurkis) siuntų svorį (laukas \"svoris\") skirtingose svorio grupėse (laukas \"svorio grupe\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2ef3811c-5465-4d5e-8dc5-235578bd255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1 užduotis: Mažiausio / Didžiausio / Vidutinio svorio skaičiavimas pagal svorio grupę ---\n",
      "Svorio statistika pagal svorio grupę:\n",
      "  Svorio grupė '<50':\n",
      "    Minimalus svoris: 0.00\n",
      "    Maksimalus svoris: 49.95\n",
      "    Vidutiniškas svoris: 5.91\n",
      "  Svorio grupė '<300':\n",
      "    Minimalus svoris: 50.00\n",
      "    Maksimalus svoris: 299.20\n",
      "    Vidutiniškas svoris: 107.82\n",
      "  Svorio grupė '>300':\n",
      "    Minimalus svoris: 300.00\n",
      "    Maksimalus svoris: 6784.00\n",
      "    Vidutiniškas svoris: 748.02\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# == Task 1: Min/Max/Avg Weight per DERIVED Weight Group ('Slenkstinis lygmuo') ==\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 1 užduotis: Mažiausio / Didžiausio / Vidutinio svorio skaičiavimas pagal svorio grupę ---\")\n",
    "\n",
    "\n",
    "# Define a function to extract weight and DERIVE the group based on the weight value.\n",
    "def derive_group_and_extract_weight(record):\n",
    "    \"\"\"\n",
    "    Gets 'svoris' from a record, converts it to float,\n",
    "    DERIVES the weight group based on the float value,\n",
    "    and returns a list containing a tuple `[(derived_group, weight_float)]`.\n",
    "    Returns an empty list `[]` if 'svoris' key is missing or value is not a valid number.\n",
    "    \"\"\"\n",
    "    weight_str = record.get('svoris') # Safely get 'svoris' value (or None if key missing)\n",
    "\n",
    "    if weight_str is not None: # Check if the 'svoris' key existed\n",
    "        try:\n",
    "            weight_float = float(weight_str) # Try converting weight string to a number\n",
    "\n",
    "            if weight_float < 50:\n",
    "                derived_group = '<50'\n",
    "            elif weight_float < 300: # If not < 50, check if it's < 300\n",
    "                derived_group = '<300'\n",
    "            else: # Otherwise, it must be >= 300\n",
    "                derived_group = '>300'\n",
    "\n",
    "\n",
    "            return [(derived_group, weight_float)] \n",
    "\n",
    "        except ValueError:\n",
    "            # If 'svoris' could not be converted to a float (e.g., it's text)\n",
    "            return [] # Return an empty list to discard this record for this task\n",
    "    else:\n",
    "        # If 'svoris' key was missing in the record\n",
    "        return [] # Return an empty list\n",
    "\n",
    "# Use `flatMap` with the NEW function to get (derived_group, weight) pairs.\n",
    "# This completely ignores the original 'svorio grupe' field from the file.\n",
    "weight_data_rdd = parsed_rdd.flatMap(derive_group_and_extract_weight) # <--- USE THE NEW FUNCTION HERE\n",
    "\n",
    "\n",
    "# Define the \"zero value\"\n",
    "initial_accumulator = (0.0, 0, float('inf'), float('-inf'))\n",
    "\n",
    "# Define the \"sequence function\"\n",
    "def merge_value_into_accumulator(acc, value):\n",
    "    current_sum, current_count, current_min, current_max = acc\n",
    "    new_sum = current_sum + value\n",
    "    new_count = current_count + 1\n",
    "    new_min = min(current_min, value)\n",
    "    new_max = max(current_max, value)\n",
    "    return (new_sum, new_count, new_min, new_max)\n",
    "\n",
    "# Define the \"combiner function\"\n",
    "def combine_accumulators(acc1, acc2):\n",
    "    sum1, count1, min1, max1 = acc1\n",
    "    sum2, count2, min2, max2 = acc2\n",
    "    combined_sum = sum1 + sum2\n",
    "    combined_count = count1 + count2\n",
    "    combined_min = min(min1, min2)\n",
    "    combined_max = max(max1, max2)\n",
    "    return (combined_sum, combined_count, combined_min, combined_max)\n",
    "\n",
    "# Perform the aggregation.\n",
    "aggregated_stats_rdd = weight_data_rdd.aggregateByKey(\n",
    "    initial_accumulator,\n",
    "    merge_value_into_accumulator,\n",
    "    combine_accumulators\n",
    ")\n",
    "\n",
    "# Define a function to format the results nicely.\n",
    "def format_weight_stats(item):\n",
    "    group, stats = item\n",
    "    total_sum, count, min_val, max_val = stats\n",
    "    avg_val = total_sum / count if count > 0 else 0.0\n",
    "    min_val = min_val if min_val != float('inf') else \"N/A\"\n",
    "    max_val = max_val if max_val != float('-inf') else \"N/A\"\n",
    "    return (group, {'min': min_val, 'max': max_val, 'avg': avg_val, 'count': count})\n",
    "\n",
    "# Apply the formatting function.\n",
    "formatted_stats_rdd = aggregated_stats_rdd.map(format_weight_stats)\n",
    "\n",
    "# Collect the results.\n",
    "weight_stats_list = formatted_stats_rdd.collect()\n",
    "\n",
    "\n",
    "# Define a helper function to assign a sort order number to each group name\n",
    "def get_group_sort_order(group_name):\n",
    "    if group_name == '<50':\n",
    "        return 0  # First\n",
    "    elif group_name == '<300':\n",
    "        return 1  # Second\n",
    "    elif group_name == '>300':\n",
    "        return 2  # Third\n",
    "    else:\n",
    "        return 99 # Put any other unexpected groups last\n",
    "\n",
    "if weight_stats_list:\n",
    "    print(\"Svorio statistika pagal svorio grupę:\")\n",
    "\n",
    "\n",
    "    # Sort the list using the custom key function BEFORE the loop\n",
    "    # The key function is applied to each item in the list (item[0] is the group name)\n",
    "    sorted_weight_stats_list = sorted(weight_stats_list, key=lambda item: get_group_sort_order(item[0]))\n",
    "\n",
    "    # Now iterate through the CUSTOM SORTED list\n",
    "    for group, stats in sorted_weight_stats_list:\n",
    "        print(f\"  Svorio grupė '{group}':\")\n",
    "        # Use isinstance to check type before formatting, avoids errors if min/max are \"N/A\"\n",
    "        print(f\"    Minimalus svoris: {stats['min']:.2f}\" if isinstance(stats['min'], float) else f\"    Min Weight: {stats['min']}\")\n",
    "        print(f\"    Maksimalus svoris: {stats['max']:.2f}\" if isinstance(stats['max'], float) else f\"    Max Weight: {stats['max']}\")\n",
    "        print(f\"    Vidutiniškas svoris: {stats['avg']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf9a0d-2e36-494c-88e6-6c4c8c67a174",
   "metadata": {},
   "source": [
    "### Tipinis lygmuo (7-8): Raskite maršrutus, kurie aplanko daugiau nei vieną geografinę zoną (laukas \"geografine zona\"). Koks tai procentas nuo visų maršrutų? Raskite atvejus, kai tai daroma ta pačia diena (laukas \"sustojimo data\"). Koks tai procentas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9f7f714f-c217-4b1f-a1d6-73ecb74fd7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Užduotis 2: Maršrutų, aplankančių kelias geografines zonas, paieška ---\n",
      "\n",
      "--- Užduotis 2 (A dalis) ---\n",
      "Rasta 339 maršrutų, aplankančių daugiau nei vieną geografinių zoną (bendroje analizėje).\n",
      "  Procentas nuo visų unikalių maršrutų: 80.33%\n",
      "\n",
      "  Maršrutai (rodomi pirmi 100): ['103', '107', '109', '110', '111', '112', '113', '114', '116', '117', '119', '127', '128', '131', '137', '138', '140', '141', '142', '143', '144', '145', '146', '148', '150', '151', '152', '153', '154', '156', '157', '160', '161', '163', '164', '165', '166', '167', '170', '171', '172', '173', '174', '175', '176', '179', '203', '204', '205', '207', '208', '209', '210', '211', '212', '213', '214', '216', '217', '218', '219', '220', '221', '222', '223', '224', '227', '228', '229', '230', '232', '234', '236', '238', '240', '241', '243', '244', '245', '246', '247', '248', '250', '251', '252', '253', '254', '259', '260', '261', '262', '263', '267', '268', '269', '280', '281', '283', '284', '285']...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Užduotis 2 (B dalis) ---\n",
      "Rasta 3012 atvejų (unikalių maršruto-datos porų), kai maršrutas tą pačią dieną aplankė daugiau nei vieną zoną.\n",
      "  Procentas nuo visų unikalių maršruto-datos porų: 39.43%\n",
      "\n",
      "  Pavyzdžiai (rodomi pirmi 10):\n",
      "    Maršrutasš: 103, Date: 2018-01-18\n",
      "    Maršrutasš: 107, Date: 2018-01-16\n",
      "    Maršrutasš: 109, Date: 2018-01-18\n",
      "    Maršrutasš: 110, Date: 2018-01-23\n",
      "    Maršrutasš: 111, Date: 2018-01-04\n",
      "    Maršrutasš: 112, Date: 2018-01-24\n",
      "    Maršrutasš: 113, Date: 2018-01-18\n",
      "    Maršrutasš: 114, Date: 2018-01-05\n",
      "    Maršrutasš: 114, Date: 2018-01-12\n",
      "    Maršrutasš: 116, Date: 2018-01-02\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# == Task 2: Routes Spanning Multiple Zones ('Tipinis lygmuo') ==\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Užduotis 2: Maršrutų, aplankančių kelias geografines zonas, paieška ---\")\n",
    "\n",
    "\n",
    "# Define a function to extract route, zone, and date from a record.\n",
    "def extract_route_zone_date_info(record):\n",
    "    \"\"\"\n",
    "    Gets 'marsrutas', 'geografine zona', and 'sustojimo data' from a record.\n",
    "    Returns a list containing a tuple `[(route, zone, date)]` if route and zone exist,\n",
    "    or an empty list `[]` otherwise. Date is included for Part B.\n",
    "    \"\"\"\n",
    "    route = record.get('marsrutas')\n",
    "    zone = record.get('geografine zona')\n",
    "    date = record.get('sustojimo data') # Can be None if missing\n",
    "\n",
    "    if route and zone: # We only need route and zone to exist for this task\n",
    "        return [(route, zone, date)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Use `flatMap` to extract the needed info.\n",
    "# Input RDD: Dictionaries\n",
    "# Output RDD (`route_zone_date_rdd`): ('102', 'Z1', '2018-01-02'), ('102', 'Z1', None), ...\n",
    "route_zone_date_rdd = parsed_rdd.flatMap(extract_route_zone_date_info)\n",
    "# Optional: Cache if the RDD is large and computations are slow\n",
    "# route_zone_date_rdd.cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Part A: Routes visiting more than one zone (overall, any day) ---\n",
    "\n",
    "# 1. Select only route and zone.\n",
    "route_zone_rdd = route_zone_date_rdd.map(lambda x: (x[0], x[1])) # (route, zone)\n",
    "\n",
    "# 2. Find the total number of UNIQUE routes (for percentage calculation)\n",
    "total_unique_routes_rdd = route_zone_rdd.map(lambda x: x[0]).distinct() # Get route IDs, find unique\n",
    "total_unique_routes_count = total_unique_routes_rdd.count()\n",
    "\n",
    "# 3. Find unique (route, zone) pairs.\n",
    "distinct_route_zone_rdd = route_zone_rdd.distinct()\n",
    "\n",
    "# 4. Group the unique zones by route.\n",
    "zones_per_route_rdd = distinct_route_zone_rdd.groupByKey()\n",
    "\n",
    "# 5. Filter to keep only routes where the number of unique zones is greater than 1.\n",
    "multi_zone_routes_grouped_rdd = zones_per_route_rdd.filter(lambda x: len(list(x[1])) > 1)\n",
    "\n",
    "# 6. Extract just the route numbers (the keys).\n",
    "multi_zone_route_ids_rdd = multi_zone_routes_grouped_rdd.map(lambda x: x[0])\n",
    "\n",
    "# 7. Collect the results (action).\n",
    "multi_zone_routes_list = multi_zone_route_ids_rdd.collect()\n",
    "\n",
    "# 8. Calculate the percentage for Part A\n",
    "percentage_part_a = (len(multi_zone_routes_list) / total_unique_routes_count * 100) if total_unique_routes_count > 0 else 0\n",
    "\n",
    "# Print Part A results\n",
    "print(\"\\n--- Užduotis 2 (A dalis) ---\")\n",
    "\n",
    "if multi_zone_routes_list:\n",
    "    print(f\"Rasta {len(multi_zone_routes_list)} maršrutų, aplankančių daugiau nei vieną geografinių zoną (bendroje analizėje).\")\n",
    "    print(f\"  Procentas nuo visų unikalių maršrutų: {percentage_part_a:.2f}%\\n\")\n",
    "    \n",
    "    routes_to_show = sorted(multi_zone_routes_list)\n",
    "    print(f\"  Maršrutai (rodomi pirmi 100): {routes_to_show[:100]}{'...' if len(routes_to_show) > 100 else ''}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Part B: Routes visiting more than one zone on the SAME day ---\n",
    "\n",
    "# 1. Filter out records where the date is missing.\n",
    "# Input RDD (`route_zone_date_rdd`): ('102', 'Z1', '2018-01-02'), ('102', 'Z1', None), ...\n",
    "rdd_with_dates = route_zone_date_rdd.filter(lambda x: x[2] is not None) # x[2] is the date\n",
    "\n",
    "# 2. Find the total number of UNIQUE (route, date) pairs (for percentage calculation)\n",
    "total_unique_route_date_rdd = rdd_with_dates.map(lambda x: (x[0], x[2])).distinct() # Get (route, date), find unique\n",
    "total_unique_route_date_count = total_unique_route_date_rdd.count()\n",
    "\n",
    "\n",
    "# 3. Create key-value pairs where the key is (route, date) and the value is zone.\n",
    "route_date_key_rdd = rdd_with_dates.map(lambda x: ((x[0], x[2]), x[1])) # Key=(route, date), Value=zone\n",
    "\n",
    "# 4. Find unique ((route, date), zone) combinations.\n",
    "distinct_route_date_zone_rdd = route_date_key_rdd.distinct()\n",
    "\n",
    "# 5. Group the unique zones by the (route, date) key.\n",
    "zones_per_route_day_rdd = distinct_route_date_zone_rdd.groupByKey()\n",
    "\n",
    "# 6. Filter to keep only (route, date) keys where the number of unique zones is > 1.\n",
    "multi_zone_same_day_grouped_rdd = zones_per_route_day_rdd.filter(lambda x: len(list(x[1])) > 1)\n",
    "\n",
    "# 7. Extract just the (route, date) keys.\n",
    "multi_zone_same_day_keys_rdd = multi_zone_same_day_grouped_rdd.map(lambda x: x[0])\n",
    "\n",
    "# 8. Collect the results (action).\n",
    "multi_zone_same_day_list = multi_zone_same_day_keys_rdd.collect()\n",
    "\n",
    "# 9. Calculate the percentage for Part B\n",
    "percentage_part_b = (len(multi_zone_same_day_list) / total_unique_route_date_count * 100) if total_unique_route_date_count > 0 else 0\n",
    "\n",
    "# Print Part B results\n",
    "print(\"\\n\\n\\n\\n--- Užduotis 2 (B dalis) ---\")\n",
    "\n",
    "if multi_zone_same_day_list:\n",
    "    print(f\"Rasta {len(multi_zone_same_day_list)} atvejų (unikalių maršruto-datos porų), kai maršrutas tą pačią dieną aplankė daugiau nei vieną zoną.\")\n",
    "    print(f\"  Procentas nuo visų unikalių maršruto-datos porų: {percentage_part_b:.2f}%\\n\")\n",
    "\n",
    "    # Sort by route first, then by date for consistent output\n",
    "    sorted_list = sorted(multi_zone_same_day_list, key=lambda item: (str(item[0]), item[1])) # Sort route as string robustly\n",
    "    print(\"  Pavyzdžiai (rodomi pirmi 10):\")\n",
    "    for route, date in sorted_list[:10]:\n",
    "        print(f\"    Maršrutas: {route}, Data: {date}\")\n",
    "\n",
    "\n",
    "# Optional: Unpersist if RDD was cached\n",
    "# route_zone_date_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0c6bb-f8e2-49ef-b32e-5b2de500dd89",
   "metadata": {},
   "source": [
    "### Puikus lygmuo (9-10): Sudarykite lentelę, kurioje matytųsi kiek pristatyta siuntų (\"siuntu skaicius\") bei aptarnauta klientų (\"Sustojimo klientu skaicius\") skirtinguose geografinėse zonose (\"geografine zona\") skirtingomis savaitės dienomis (\"sustojimo savaites diena\"). Palyginkite užduoties sprendimo laiką su MapReduce versiją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "49a18cdf-4b77-4db5-83e7-c85e24c47225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3 užduotis: Siuntų ir klientų agregavimas pagal zoną ir savaitės dieną ---\n",
      "Bendras siuntų ir aptarnautų klientų skaičius pagal zoną ir savaitės dieną:\n",
      "\n",
      "Geogr. zona   Savaitės d.  Viso siuntų    Viso klientų\n",
      "Z1            1            14092          6867             \n",
      "Z1            2            21024          9979             \n",
      "Z1            3            20494          10106            \n",
      "Z1            4            16775          8110             \n",
      "Z1            5            14653          7616             \n",
      "Z1            6            302            127              \n",
      "Z2            1            2910           1927             \n",
      "Z2            2            4933           3045             \n",
      "Z2            3            5335           3087             \n",
      "Z2            4            3603           2234             \n",
      "Z2            5            4073           2340             \n",
      "Z2            6            23             11               \n",
      "Z3            1            2896           1831             \n",
      "Z3            2            4206           2874             \n",
      "Z3            3            4531           2914             \n",
      "Z3            4            3036           2088             \n",
      "Z3            5            2865           1958             \n",
      "Z3            6            7              5                \n",
      "\n",
      "Analysis complete. Stopping Spark session.\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# == Task 3: Shipments/Customers per Zone per Weekday ('Puikus lygmuo') ==\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3 užduotis: Siuntų ir klientų agregavimas pagal zoną ir savaitės dieną ---\")\n",
    "\n",
    "\n",
    "# Define a function to extract zone, weekday, shipments, and customers.\n",
    "def extract_zone_weekday_counts(record):\n",
    "    \"\"\"\n",
    "    Gets 'geografine zona', 'sustojimo savaites diena', 'siuntu skaicius',\n",
    "    and 'Sustojimo klientu skaicius'.\n",
    "    Converts counts to integers.\n",
    "    Returns a list `[ ((zone, weekday), (shipments, customers)) ]` if successful,\n",
    "    or `[]` otherwise. The key is a tuple (zone, weekday), value is a tuple (shipments, customers).\n",
    "    \"\"\"\n",
    "    zone = record.get('geografine zona')\n",
    "    weekday_str = record.get('sustojimo savaites diena')\n",
    "    shipments_str = record.get('siuntu skaicius')\n",
    "    customers_str = record.get('Sustojimo klientu skaicius')\n",
    "\n",
    "    # Check if all required fields are present\n",
    "    if zone and weekday_str and shipments_str and customers_str:\n",
    "        try:\n",
    "            # Convert numbers from strings to integers\n",
    "            weekday = int(weekday_str)\n",
    "            shipments = int(shipments_str)\n",
    "            customers = int(customers_str)\n",
    "            # Return the data as a list containing one key-value pair tuple\n",
    "            # Key: (zone, weekday)  Value: (shipments, customers)\n",
    "            return [((zone, weekday), (shipments, customers))]\n",
    "        except ValueError:\n",
    "            # If any number conversion fails\n",
    "            return []\n",
    "    else:\n",
    "        # If any key is missing\n",
    "        return []\n",
    "\n",
    "# Use `flatMap` to extract the needed info and filter out bad records.\n",
    "# Input RDD: Dictionaries\n",
    "# Output RDD (`counts_data_rdd`): ( ('Z1', 2), (1, 1) ), ( ('Z1', 2), (4, 1) ), ...\n",
    "counts_data_rdd = parsed_rdd.flatMap(extract_zone_weekday_counts)\n",
    "\n",
    "# Use `reduceByKey` to sum the shipments and customers for each unique key (zone, weekday).\n",
    "# `reduceByKey` is a transformation that combines values for the same key using an associative function.\n",
    "# Here, the function takes two value tuples `a = (shipments1, customers1)` and `b = (shipments2, customers2)`\n",
    "# and returns a new tuple `(shipments1+shipments2, customers1+customers2)`.\n",
    "# Input RDD: ( ('Z1', 2), (1, 1) ), ( ('Z1', 2), (4, 1) ), ( ('Z1', 3), (2, 1) ), ...\n",
    "# Output RDD (`aggregated_counts_rdd`): ( ('Z1', 2), (5, 2) ), ( ('Z1', 3), (2, 1) ), ...\n",
    "aggregated_counts_rdd = counts_data_rdd.reduceByKey(\n",
    "    lambda tuple_a, tuple_b: (tuple_a[0] + tuple_b[0], tuple_a[1] + tuple_b[1])\n",
    "    # tuple_a[0] is shipments in first tuple, tuple_b[0] is shipments in second tuple\n",
    "    # tuple_a[1] is customers in first tuple, tuple_b[1] is customers in second tuple\n",
    ")\n",
    "\n",
    "# Sort the results by the key (zone, then weekday) for easier reading.\n",
    "# `.sortByKey()` is a transformation. True means ascending order (default).\n",
    "# Input RDD: ( ('Z1', 3), (2, 1) ), ( ('Z1', 2), (5, 2) ), ( ('Z2', 2), (10, 8) ), ...\n",
    "# Output RDD (`sorted_counts_rdd`): ( ('Z1', 2), (5, 2) ), ( ('Z1', 3), (2, 1) ), ( ('Z2', 2), (10, 8) ), ...\n",
    "sorted_counts_rdd = aggregated_counts_rdd.sortByKey(ascending=True)\n",
    "\n",
    "# Collect the sorted results (action). Again, assume the final result is small.\n",
    "final_counts_list = sorted_counts_rdd.collect()\n",
    "\n",
    "# Print the collected results in a table format\n",
    "if final_counts_list:\n",
    "    print(\"Bendras siuntų ir aptarnautų klientų skaičius pagal zoną ir savaitės dieną:\\n\")\n",
    "    print(\"Geogr. zona   Savaitės d.  Viso siuntų    Viso klientų\")\n",
    "    \n",
    "    # Iterate through the sorted list and print each row\n",
    "    for (zone, weekday), (shipments, customers) in final_counts_list:\n",
    "        # Format each part to fit nicely in the columns\n",
    "        # <: left-align, >: right-align, number: width\n",
    "        print(f\"{zone:<12}  {weekday:<11}  {shipments:<13}  {customers:<16} \")\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# --- Stop Spark Session ---\n",
    "# It's important to stop the SparkSession to release resources.\n",
    "print(\"\\nAnalysis complete. Stopping Spark session.\")\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
